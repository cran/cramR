<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Cram Bandit Helpers</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Cram Bandit Helpers</h1>



<div id="what-is-this-article-about" class="section level1">
<h1>üåü What is this article about?</h1>
<p>In order to use <code>cram_bandit()</code>, users must supply a
matrix of <strong>action selection probabilities</strong> <span class="math inline">\(\pi_t(X_j, A_j)\)</span> for each combination of
policy update <span class="math inline">\(t\)</span> and context <span class="math inline">\(j\)</span> in the historical dataset.</p>
<p>While some environments log these probabilities directly, many
contextual bandit libraries (such as <a href="https://github.com/Nth-iteration-labs/contextual"><code>contextual</code></a>)
only store <strong>policy parameters</strong> (e.g., regression
coefficients) without explicit probability tracking.</p>
<p>This article explains how <strong>Cram Bandit Helpers</strong>
reconstruct <span class="math inline">\(\pi_t(X_j, A_j)\)</span> from
these parameters for common policies:</p>
<table>
<colgroup>
<col width="30%" />
<col width="70%" />
</colgroup>
<thead>
<tr class="header">
<th>Policy Type</th>
<th>Class Name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Epsilon-Greedy</td>
<td><code>BatchContextualEpsilonGreedyPolicy</code></td>
</tr>
<tr class="even">
<td>LinUCB Disjoint with <span class="math inline">\(\varepsilon\)</span>-greedy exploration</td>
<td><code>BatchLinUCBDisjointPolicyEpsilon</code></td>
</tr>
<tr class="odd">
<td>Thompson Sampling</td>
<td><code>BatchContextualLinTSPolicy</code></td>
</tr>
</tbody>
</table>
<p>Both <strong>theoretical formulas</strong> and <strong>practical code
snippets</strong> are provided.</p>
<hr />
</div>
<div id="policy-parameters-explained" class="section level1">
<h1>üõ†Ô∏èPolicy parameters explained</h1>
<p>When using linear bandit algorithms like Epsilon-Greedy, LinUCB, or
Thompson Sampling, each arm <span class="math inline">\(k\)</span>
maintains <strong>summary statistics</strong> (parameters) to estimate
the expected reward:</p>
<ul>
<li><p><span class="math inline">\(A_k\)</span> is the <strong>Gram
matrix</strong>:<br />
<span class="math display">\[
A_k = X_k^T X_k
\]</span> where <span class="math inline">\(X_k\)</span> is the matrix
of feature vectors (contexts) for all rounds where arm <span class="math inline">\(k\)</span> was selected.<br />
‚ûî <strong>Interpretation</strong>: <span class="math inline">\(A_k\)</span> captures the amount of information
(and correlation structure) about the features for arm <span class="math inline">\(k\)</span>. It plays the role of a ‚Äúfeature
covariance matrix.‚Äù</p></li>
<li><p><span class="math inline">\(b_k\)</span> is the <strong>response
vector</strong>:<br />
<span class="math display">\[
b_k = X_k^T y_k
\]</span> where <span class="math inline">\(y_k\)</span> are the
observed rewards for arm <span class="math inline">\(k\)</span>.<br />
‚ûî <strong>Interpretation</strong>: <span class="math inline">\(b_k\)</span> captures the relationship between the
observed rewards and the contexts for arm <span class="math inline">\(k\)</span>.</p></li>
</ul>
<p>These sufficient statistics allow the policy to compute the
<strong>Least Squares estimate</strong> for the reward model:</p>
<p><span class="math display">\[
\theta_k = A_k^{-1} b_k
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\theta_k\)</span> is the estimated
coefficient vector that predicts the expected reward of arm <span class="math inline">\(k\)</span> as a function of the context.</li>
</ul>
<p>Thus:</p>
<ul>
<li><span class="math inline">\(A_k\)</span> tells us <strong>how
confident</strong> we are about <span class="math inline">\(\theta_k\)</span> (smaller eigenvalues of <span class="math inline">\(A_k\)</span> imply more uncertainty).</li>
<li><span class="math inline">\(b_k\)</span> provides the
<strong>observed signal</strong> (reward-weighted context features) to
fit <span class="math inline">\(\theta_k\)</span>.</li>
</ul>
<p>The policy selects an action based on the <span class="math inline">\(\theta_k\)</span> of each arm <span class="math inline">\(k\)</span> and then observe the reward associated
with this choice, which is used to update the parameters <span class="math inline">\(A_k\)</span> and <span class="math inline">\(b_k\)</span> of the policy.</p>
<hr />
</div>
<div id="epsilon-greedy-policy" class="section level1">
<h1>‚ú® Epsilon-Greedy Policy</h1>
<div id="theoretical-computation" class="section level3">
<h3>ü§ñ Theoretical computation</h3>
<p>In Epsilon-Greedy, with exploration rate <span class="math inline">\(\varepsilon\)</span>, the probability of selecting
one of the best arms is:</p>
<p><span class="math display">\[ P(A_t | X_t) = (1 - \varepsilon) \times
\frac{1}{\# \text{best arms}} + \varepsilon \times \frac{1}{K}
\]</span></p>
<p>While the probability of selecting an arm that is not among the best
arms is:</p>
<p><span class="math display">\[ P(A_t | X_t) = \varepsilon \times
\frac{1}{K} \]</span></p>
<p>where:</p>
<ul>
<li>Best arms are those with maximal estimated rewards.</li>
<li><span class="math inline">\(K\)</span> is the total number of
available arms.</li>
</ul>
<p>We define the least squares estimate as:</p>
<p><span class="math display">\[ \theta_k = A_k^{-1} b_k \quad
\text{(Least Squares estimate)} \]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(A_k\)</span> is the Gram matrix <span class="math inline">\(X_k^T X_k\)</span></li>
<li><span class="math inline">\(b_k\)</span> is the vector <span class="math inline">\(X_k^T Y_k\)</span></li>
</ul>
<p>Best arms are identified via the estimated expected reward:</p>
<p><span class="math display">\[ \text{Expected reward} = X_t^T \theta_k
\]</span></p>
</div>
<div id="code-helper" class="section level3">
<h3>üìä Code helper</h3>
<p>In <code>cramR</code>, this is implemented by:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">get_proba_c_eps_greedy</span>(eps, A_list, b_list, contexts, chosen_arms)</span></code></pre></div>
<p>This function:</p>
<ul>
<li>Computes <span class="math inline">\(\theta_k\)</span> for each
arm</li>
<li>Calculates expected rewards <span class="math inline">\(X_t^T
\theta_k\)</span></li>
<li>Identifies the best arms</li>
<li>Applies the above formula</li>
</ul>
<hr />
</div>
</div>
<div id="linucb-disjoint-policy-with-varepsilon-greedy" class="section level1">
<h1>üî¢ LinUCB Disjoint Policy with <span class="math inline">\(\varepsilon\)</span>-Greedy</h1>
<div id="theoretical-computation-1" class="section level3">
<h3>ü§ñ Theoretical computation</h3>
<p>LinUCB selects arms based on <strong>Upper Confidence Bounds
(UCBs)</strong>:</p>
<p><span class="math display">\[ \text{UCB}_k(X_t) = \mu_k(X_t) + \alpha
\sigma_k(X_t) \]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mu_k(X_t) = X_t^T
\theta_k\)</span></li>
<li><span class="math inline">\(\sigma_k(X_t) = \sqrt{X_t^T A_k^{-1}
X_t}\)</span> measures uncertainty</li>
<li><span class="math inline">\(\alpha\)</span> controls the exploration
strength</li>
</ul>
<p>The action probabilities follow the same structure as Epsilon-Greedy
but with UCB scores instead of plain expected rewards i.e.¬†the
probability to select one of the best arms is:</p>
<p><span class="math display">\[ P(A_t | X_t) = (1 - \varepsilon) \times
\frac{1}{\# \text{best arms}} + \varepsilon \times \frac{1}{K}
\]</span></p>
<p>While the probability to select an arm that is not among the best
arms is:</p>
<p><span class="math display">\[ P(A_t | X_t) = \varepsilon \times
\frac{1}{K} \]</span></p>
<p>where ‚Äúbest arms‚Äù are those with highest UCB scores.</p>
</div>
<div id="code-helper-1" class="section level3">
<h3>üìä Code helper</h3>
<p>In <code>cramR</code>, this is implemented by:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="fu">get_proba_ucb_disjoint</span>(alpha, eps, A_list, b_list, contexts, chosen_arms)</span></code></pre></div>
<p>This function:</p>
<ul>
<li>Computes <span class="math inline">\(\theta_k\)</span></li>
<li>Computes <span class="math inline">\(\mu_k(X_t)\)</span> and <span class="math inline">\(\sigma_k(X_t)\)</span></li>
<li>Identifies arms maximizing <span class="math inline">\(\text{UCB}_k(X_t)\)</span></li>
<li>Applies the Epsilon-Greedy selection formula</li>
</ul>
<hr />
</div>
</div>
<div id="thompson-sampling-lints" class="section level1">
<h1>ü§ì Thompson Sampling (LinTS)</h1>
<div id="theoretical-computation-2" class="section level3">
<h3>ü§ñ Theoretical computation</h3>
<p>In Thompson Sampling, actions are sampled according to posterior
draws and the action associated with the maximum value is chosen. The
probability that the arm <span class="math inline">\(A_t\)</span> is
optimal is:</p>
<p><span class="math display">\[ P(A_t | X_t) = \mathbb{P}\left(
\theta_{A_t}^T X_t &gt; \theta_{k}^T X_t \quad \forall k \neq A_t
\right) \]</span></p>
<p>where <span class="math inline">\(\theta_k \sim \mathcal{N}(A_k^{-1}
b_k, \sigma^2 A_k^{-1})\)</span>.</p>
<p>This requires <strong>computing a multivariate probability</strong>,
which we approximate via <strong>adaptive numerical
integration</strong>.</p>
</div>
<div id="code-helper-2" class="section level3">
<h3>üìä Code helper</h3>
<p>In <code>cramR</code>, this is implemented by:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="fu">get_proba_thompson</span>(sigma, A_list, b_list, contexts, chosen_arms)</span></code></pre></div>
<p>This function:</p>
<ul>
<li>Computes posterior means and variances</li>
<li>Integrates over the space where chosen arm <span class="math inline">\(A_t\)</span> has the highest sampled reward</li>
<li>Returns clipped probabilities for numerical stability</li>
</ul>
<hr />
</div>
</div>
<div id="practical-workflow" class="section level1">
<h1>üë®‚Äçüíª Practical Workflow</h1>
<p>When using your bandit policy in practice:</p>
<ol style="list-style-type: decimal">
<li>Record action choices, contexts, and policy parameters (e.g., <span class="math inline">\(A\)</span>, <span class="math inline">\(b\)</span>)</li>
<li>Calculate the action selection probabilities. If your policy is
within the ones presented above, please feel free to rely on our helper
functions to build <span class="math inline">\(\pi\)</span>.</li>
<li>Feed <code>pi</code>, <code>arm</code>, and <code>reward</code> into
<code>cram_bandit()</code> for evaluation of your policy.</li>
</ol>
<hr />
</div>
<div id="estimand-calculation-in-cram_bandit_sim" class="section level1">
<h1>üß™ Estimand Calculation in <code>cram_bandit_sim()</code></h1>
<p>The following only concerns the simulation framework we implemented
for benchmarking purposes.</p>
<p>Once the policies are reconstructed, we compute their true expected
value ‚Äî referred to as the estimand ‚Äî by applying the learned policy to
independent contexts and evaluating it against the known reward function
used in the simulation.</p>
<p>This is done via:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="fu">compute_estimand</span>(data_group, list_betas, policy, policy_name, batch_size, bandit)</span></code></pre></div>
<p>Accurately computing the estimand is critical for properly assessing
the bias and confidence interval coverage of the Cram estimate in our
simulations.</p>
</div>
<div id="useful-links" class="section level1">
<h1>üìÇ Useful Links</h1>
<ul>
<li><a href="https://github.com/Nth-iteration-labs/contextual"><code>contextual</code></a>
package: original framework</li>
<li><code>cram_bandit()</code>: Cram evaluation for contextual
bandits</li>
<li><code>cram_bandit_sim()</code>: Full simulation engine with
automatic pi estimation</li>
</ul>
<hr />
</div>
<div id="acknowledgments" class="section level1">
<h1>üåü Acknowledgments</h1>
<p>These helper functions were designed to faithfully reconstruct action
probabilities for the policies implemented in <a href="https://github.com/Nth-iteration-labs/contextual"><code>contextual</code></a>,
while enabling reproducible Cram-based evaluation.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
